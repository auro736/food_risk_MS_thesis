{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import six\n",
    "import shap\n",
    "\n",
    "from TRC.models import ModelForWeightedSequenceClassification\n",
    "from common_utils import extract_from_dataframe, mask_batch_seq_generator\n",
    "from TRC.utils import evaluate, pad_sequences, simple_tokenize\n",
    "\n",
    "from error_analysis_TRC import load_local_model, create_weight, prepare_data, tokenize_with_new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSIGN_WEIGHT = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'roberta-large'\n",
    "\n",
    "model_path = '/home/cc/rora_tesi_new/log/log_TRC/roberta-large/bertweet-seq/20_epoch/data/True_weight/42_seed/saved-model/pytorch_model.bin'\n",
    "config_path = '/home/cc/rora_tesi_new/log/log_TRC/roberta-large/bertweet-seq/20_epoch/data/True_weight/42_seed/saved-model/config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/home/cc/rora_tesi_new/data/train.p'\n",
    "test_data_path = '/home/cc/rora_tesi_new/data/test.p'\n",
    "\n",
    "need_columns = ['tweet_tokens', 'sentence_class']\n",
    "\n",
    "_, Y_train = prepare_data(train_data_path, need_columns)\n",
    "X_test_raw, Y_test = prepare_data(test_data_path, need_columns)\n",
    "\n",
    "test_batch_size = Y_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, normalization = True)\n",
    "model = load_local_model(model_path, config_path, device, model_name)\n",
    "model = model.to(device)\n",
    "labels = sorted(model.config.label2id, key=model.config.label2id.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "if ASSIGN_WEIGHT:\n",
    "    class_weight = create_weight(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prova(X):\n",
    "    X_test, masks_test = tokenize_with_new_mask(X, 128, tokenizer)\n",
    "    num_batches = X_test.shape[0] // len(X)\n",
    "    print('aaaa',num_batches)\n",
    "    \n",
    "    test_batch_generator = mask_batch_seq_generator(X_test, Y_test, masks_test, 1)\n",
    "    logits, y_true, test_loss, test_auc, test_acc, test_tn, test_fp, test_fn, test_tp, test_precision, test_recall, test_s_pred = evaluate(model,\n",
    "                                                                                                test_batch_generator,\n",
    "                                                                                                num_batches, device,\n",
    "                                                                                                class_weight )\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaa 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7821, -1.1009]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova(X_test_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(prova, tokenizer, output_names=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@USER', 'As', 'much', 'fun', 'as', 'I', 'can', '.', 'Woke', 'up', 'with', 'food', 'poisoning', 'or', 'stomach', 'flu', '.', 'Been', 'bugging', 'me', 'all', 'day', '#tmi', 'Almost', 'done', 'driving', 'for', 'the', 'day']\n"
     ]
    }
   ],
   "source": [
    "print(X_test_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaa 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaa 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaa 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.93it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The model produced 1 output rows when given 2 input rows! Check the implementation of the model you provided for errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m shap_values \u001b[39m=\u001b[39m explainer(X_test_raw[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/venv/rora_venv/lib/python3.8/site-packages/shap/explainers/_partition.py:136\u001b[0m, in \u001b[0;36mPartition.__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_evals\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, fixed_context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, main_effects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, error_bounds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m              outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Explain the output of the model on the given arguments.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\n\u001b[1;32m    137\u001b[0m         \u001b[39m*\u001b[39;49margs, max_evals\u001b[39m=\u001b[39;49mmax_evals, fixed_context\u001b[39m=\u001b[39;49mfixed_context, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    138\u001b[0m         outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent\n\u001b[1;32m    139\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/rora_venv/lib/python3.8/site-packages/shap/explainers/_explainer.py:266\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     feature_names \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(args))]\n\u001b[1;32m    265\u001b[0m \u001b[39mfor\u001b[39;00m row_args \u001b[39min\u001b[39;00m show_progress(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39margs), num_rows, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m explainer\u001b[39m\u001b[39m\"\u001b[39m, silent):\n\u001b[0;32m--> 266\u001b[0m     row_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain_row(\n\u001b[1;32m    267\u001b[0m         \u001b[39m*\u001b[39;49mrow_args, max_evals\u001b[39m=\u001b[39;49mmax_evals, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds,\n\u001b[1;32m    268\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size, outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     values\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    271\u001b[0m     output_indices\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39moutput_indices\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/venv/rora_venv/lib/python3.8/site-packages/shap/explainers/_partition.py:184\u001b[0m, in \u001b[0;36mPartition.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(out_shape)\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdvalues \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(out_shape)\n\u001b[0;32m--> 184\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mowen(fm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_curr_base_value, f11, max_evals \u001b[39m-\u001b[39;49m \u001b[39m2\u001b[39;49m, outputs, fixed_context, batch_size, silent)\n\u001b[1;32m    186\u001b[0m \u001b[39m# if False:\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m#     if self.multi_output:\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m#         return [self.dvalues[:,i] for i in range(self.dvalues.shape[1])], oinds\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m# drop the interaction terms down onto self.values\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues[:] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdvalues\n",
      "File \u001b[0;32m~/venv/rora_venv/lib/python3.8/site-packages/shap/explainers/_partition.py:296\u001b[0m, in \u001b[0;36mPartition.owen\u001b[0;34m(self, fm, f00, f11, max_evals, output_indexes, fixed_context, batch_size, silent)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39m# run the batch\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch_args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 296\u001b[0m     fout \u001b[39m=\u001b[39m fm(batch_masks)\n\u001b[1;32m    297\u001b[0m     \u001b[39mif\u001b[39;00m output_indexes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m         fout \u001b[39m=\u001b[39m fout[:,output_indexes]\n",
      "File \u001b[0;32m~/venv/rora_venv/lib/python3.8/site-packages/shap/utils/_masked_model.py:67\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_full_masking_call(full_masks, zero_index\u001b[39m=\u001b[39mzero_index, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     66\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_full_masking_call(masks, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n",
      "File \u001b[0;32m~/venv/rora_venv/lib/python3.8/site-packages/shap/utils/_masked_model.py:145\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    143\u001b[0m     joined_masked_inputs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([np\u001b[39m.\u001b[39mconcatenate(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m all_masked_inputs])\n\u001b[1;32m    144\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39mjoined_masked_inputs)\n\u001b[0;32m--> 145\u001b[0m     _assert_output_input_match(joined_masked_inputs, outputs)\n\u001b[1;32m    146\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(outputs)\n\u001b[1;32m    147\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(all_outputs)\n",
      "File \u001b[0;32m~/venv/rora_venv/lib/python3.8/site-packages/shap/utils/_masked_model.py:268\u001b[0m, in \u001b[0;36m_assert_output_input_match\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_output_input_match\u001b[39m(inputs, outputs):\n\u001b[0;32m--> 268\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(outputs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(inputs[\u001b[39m0\u001b[39m]), \\\n\u001b[1;32m    269\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe model produced \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(outputs)\u001b[39m}\u001b[39;00m\u001b[39m output rows when given \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(inputs[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m input rows! Check the implementation of the model you provided for errors.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model produced 1 output rows when given 2 input rows! Check the implementation of the model you provided for errors."
     ]
    }
   ],
   "source": [
    "\n",
    "shap_values = explainer(X_test_raw[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rora_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
